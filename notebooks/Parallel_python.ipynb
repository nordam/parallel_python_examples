{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "import sys\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel processing with python\n",
    "\n",
    "## A brief introduction\n",
    "\n",
    "#### Tor Nordam, SeaLab Python User Groups, May 31, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline (not in order)\n",
    "\n",
    "* A pedagogical example of parallelisation: Coffee brewing\n",
    "* Types of parallelisation:\n",
    " * Trivial parallelisation\n",
    " * Shared memory parallelisation\n",
    " * Distributed memory parallelisation\n",
    "* Relevant tools:\n",
    " * GNU parallel\n",
    " * The `multiprocessing` and `concurrent.futures` modules\n",
    " * Numba\n",
    " * MPI for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A pedagogical example: Coffee brewing\n",
    "\n",
    "A serial approach to coffee brewing:\n",
    "\n",
    "<div style=\"width:100%;align:center\">\n",
    "<img src=\"coffee_serial.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "Can this be sped up by parallelisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes!\n",
    "\n",
    "<div style=\"width:100%;align:center\">\n",
    "<img src=\"coffee_parallel1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "Can we speed up up even more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A pedagogical example: Coffee brewing\n",
    "\n",
    "<div style=\"width:100%;align:center\">\n",
    "<img src=\"coffee_parallel1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "Can we speed up up even more? ...maybe not?\n",
    "\n",
    "<div style=\"width:100%;align:center\">\n",
    "<img src=\"coffee_parallel2.png\" alt=\"Drawing\" style=\"width: 620px;\"/>\n",
    "</div>\n",
    "\n",
    "The example illustrates two important points in parallelisation:\n",
    "* Dependencies: ome task must be complete before others can start)\n",
    "* Overhead (sometimes the extra work related to parallelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of parallelisation -- Trivial parallelisation\n",
    "\n",
    "* Typical use case: You have a large number of independent tasks\n",
    " * Run a script on many different inputs (e.g. analysing many images)\n",
    " * Monte Carlo simulation with many random inputs\n",
    " * ...\n",
    " \n",
    "* Can be parallelised by simply running several copies of the script at once\n",
    "* Also known as embarrassingly parallel problems\n",
    "\n",
    "* Caveats: Will not work so well if each copy of the script uses a lot of memory and/or disk/network access\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- GNU parallel\n",
    "\n",
    "Trivial parallelisation is often best done without modifying your python code.\n",
    "\n",
    "* Assumptions:\n",
    " * A script, which reads some input (or creates random input)\n",
    " * Output is written to a unique file for each copy\n",
    "\n",
    "\n",
    "* The command line tool `parallel` is helpful here:\n",
    " * Makes it easy to run the script once for each input\n",
    " * Can automatically keep $N$ copies running simultaneously\n",
    " * Also has advanced features that can run scripts across several machines on a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A small digression: `jot`\n",
    "\n",
    "`jot` is a handy command line tool to, for example, \n",
    "* generate sequences of integers\n",
    "\n",
    "```shell\n",
    "> jot 2\n",
    "1\n",
    "2\n",
    "```\n",
    "* generate random integers\n",
    "\n",
    "```shell\n",
    "> jot -r 2\n",
    "34\n",
    "82\n",
    "```\n",
    "\n",
    "* generate random 17-digit floats between 0 and 1\n",
    "\n",
    "```bash\n",
    "> jot -r -p17 1 0 1\n",
    "0.86810522642917931\n",
    "0.61719403299503028\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- GNU parallel\n",
    "\n",
    "<div style=\"width:100%;align:center\">\n",
    "<img src=\"parallel.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "https://www.gnu.org/software/parallel/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- GNU Parallel\n",
    "\n",
    "#### Example 1: Processing a folder full of images\n",
    "\n",
    "Assumptions:\n",
    "* The script `analyse.py` takes one filename as argument\n",
    "* This is running on a machine with 8 cores\n",
    "\n",
    "```shell\n",
    "> ls *.png | parallel -n1 -P8 python analyse.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- GNU Parallel\n",
    "\n",
    "#### Example 2: Running 1000 ensemble members in a Monte Carlo simulation\n",
    "\n",
    "Assumptions:\n",
    "* The script `montecarlo.py` uses different random numbers each time\n",
    "* Output is written to a unique location each time\n",
    "* This is running on a machine with 8 cores\n",
    "\n",
    "```shell\n",
    "> jot 1000 | parallel -n1 -P8 python montecarlo.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- The multiprocessing module\n",
    "\n",
    "Typical use case: You have a large number of independent tasks\n",
    "* Run a function on many different inputs (e.g. analysing many images)\n",
    "* Monte Carlo simulation with many random inputs\n",
    "* Often quite similar to the cases where you would use GNU parallel\n",
    "\n",
    "Properties:\n",
    "* Runs each copy of a function in a separate process\n",
    "* No shared memory between processes (all data must be input)\n",
    "\n",
    "Use `multiprocessing.pool` to iterate over a list of inputs, keeping $N$ processes running until all inputs have been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "# For some reason, multiprocessing does not work with functions\n",
    "# defined inside a notebook, therefore importing this from a file\n",
    "from examples import wait_function\n",
    "\n",
    "# List of inputs for which to run simulation\n",
    "parameters = np.arange(4)\n",
    "\n",
    "# Using a pool of 2 processes will keep\n",
    "# 2 copies of the function running at all times,\n",
    "# until the function has been run for each parameter\n",
    "with Pool(2) as pool:\n",
    "    output = pool.map(wait_function, parameters)\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- The `concurrent.futures` module\n",
    "\n",
    "* Introduced in Python 3.2\n",
    "* Can do essentially the same things as multiprocessing\n",
    "* Provides a common interface for asyncronous execution with either threads or processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# For some reason, multiprocessing does not work with functions\n",
    "# defined inside a notebook, therefore importing this from a file\n",
    "from examples import wait_function\n",
    "\n",
    "# List of inputs for which to run simulation\n",
    "parameters = np.arange(4)\n",
    "\n",
    "# Using a pool of 2 processes will keep\n",
    "# 2 copies of the function running at all times,\n",
    "# until the function has been run for each parameter\n",
    "with ProcessPoolExecutor(2) as pool:\n",
    "    output = pool.map(wait_function, parameters)\n",
    "    \n",
    "# output is a generator containing the return values\n",
    "for o in output:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of parallelisation -- Shared memory parallelisation\n",
    "\n",
    "Typical use case:\n",
    "* You want to run one large simulation\n",
    "* The simulation fits in memory on one machine\n",
    "\n",
    "\n",
    "Shared memory parallelisation means:\n",
    "* Several CPU cores work on a common, shared memory\n",
    "* Typically each CPU works on a part of a large array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of parallelisation -- Shared memory parallelisation\n",
    "\n",
    "\n",
    "In C and Fortran, OpenMP is the typical example of shared memory parallelisation\n",
    "\n",
    "```Fortran\n",
    "do it = 2, Nt\n",
    "    !$OMP PARALLEL DO\n",
    "    do i = 2, Nx-1\n",
    "        U(i, it) = U(i, it-1) + alpha*(U(i+1, it-1) - 2*U(i, it-1) + U(i-1, it-1))\n",
    "    end do\n",
    "    !$OMP END PARALLEL DO\n",
    "end do\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- Numba\n",
    "\n",
    "* In Python, shared memory parallelisation is not completely straightforward due to the GIL (Global Interpreter Lock).\n",
    "* Numba provides `prange`, which is quite similar to `$OMP PARALLEL DO`.\n",
    "* https://numba.pydata.org/numba-doc/latest/user/parallel.html\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "@jit(nopython=True, parallel=True)\n",
    "def diffusion():\n",
    "    ...\n",
    "    for i in prange(1, Nx-1):\n",
    "        C_ = C[i] + alpha*(C[i+1] - 2*C[i] + C[i-1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4ed1f8c29123>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Check timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C = diffusion(10)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2325\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2326\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2327\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2328\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4ed1f8c29123>\u001b[0m in \u001b[0;36mdiffusion\u001b[0;34m(Nt)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Loop over array elements (space, parallel for-loop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mC_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# A neat trick below, see link for details\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# https://stackoverflow.com/questions/72431361/poor-performance-from-numba-prange#comment127966065_72433087\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numba import jit, prange\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def diffusion(Nt):\n",
    "    alpha = 0.49\n",
    "    x = np.linspace(0, 1, 10000000)\n",
    "    # Initial condition\n",
    "    C = 1/(0.25*np.sqrt(2*np.pi)) * np.exp(-0.5*((x-0.5)/0.25)**2)\n",
    "    # Temporary work array\n",
    "    C_ = np.zeros_like(C)\n",
    "    # Loop over time (normal for-loop)\n",
    "    for j in range(Nt):\n",
    "        # Loop over array elements (space, parallel for-loop)\n",
    "        for i in prange(1, len(C)-1):\n",
    "            C_[i] = C[i] + alpha*(C[i+1] - 2*C[i] + C[i-1])\n",
    "        # A neat trick below, see link for details\n",
    "        # https://stackoverflow.com/questions/72431361/poor-performance-from-numba-prange#comment127966065_72433087\n",
    "        C, C_ = C_, C\n",
    "    return C\n",
    "\n",
    "# Run once to just-in-time compile\n",
    "C = diffusion(1)\n",
    "\n",
    "# Check timing\n",
    "%timeit C = diffusion(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of parallelisation -- Distributed memory parallelisation\n",
    "\n",
    "* Typical use case:\n",
    " * You want to run one large simulation\n",
    " * The problem does not fit in memory on one machine\n",
    " * (or it takes so long to run that you want to use additional CPUs)\n",
    " \n",
    "* This type of parallel computing is usually what is meant by HPC (High-Perfomance Computing)\n",
    "* Often implemented with MPI (Message Passing Interface)\n",
    "* (but see also coarrays in Fortran, and Unified Parallel C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Relevant tools -- MPI for Python\n",
    "\n",
    "* The `mpi4py` module provides a Python wrapper for MPI\n",
    "* Syntax is somewhat similar to MPI in Fortran or C\n",
    "* Designed to work efficiently with numpy arrays\n",
    "\n",
    "\n",
    "* In MPI, there is no shared memory\n",
    "* Use `mpirun` to launch $N$ copies of your program\n",
    "* Communication between processes must be handled explicitly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trivial example: Parallel hello world\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Initialise MPI communicator\n",
    "comm = MPI.COMM_WORLD\n",
    "MPI_rank = comm.Get_rank()\n",
    "MPI_size = comm.Get_size()\n",
    "\n",
    "# Print rank number, and exit\n",
    "print(f'Hello, this is rank {MPI_rank} of {MPI_size}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Less trivial example: Parallel 2D diffusion solver\n",
    "\n",
    "* Split domain into smaller pieces\n",
    "* One process (rank) will handle each piece\n",
    "* Must write explicit communication to exchange boundaries (halo swap)\n",
    "* This is an example of the SPMD paradigm (Single Program, Multiple Data)\n",
    "\n",
    "\n",
    "<div style=\"width:100%;align:center\">\n",
    "<img src=\"2D_diffusion.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "\n",
    "\n",
    "def FTCS(C, D, dt, dx):\n",
    "    # One iteration with the FTCS method\n",
    "    # Interior points only\n",
    "    alpha = D*dt/dx**2\n",
    "    C[1:-1, 1:-1] = C[1:-1, 1:-1] + alpha*(\n",
    "         C[2:  , 1:-1] + C[ :-2, 1:-1]\n",
    "        +C[1:-1, 2:  ] + C[1:-1,  :-2]\n",
    "        -4*C[1:-1, 1:-1]\n",
    "    )\n",
    "    return C\n",
    "\n",
    "\n",
    "# Initialise MPI communicator\n",
    "comm = MPI.COMM_WORLD\n",
    "MPI_rank = comm.Get_rank()\n",
    "MPI_size = comm.Get_size()\n",
    "\n",
    "if MPI_rank == 0:\n",
    "    tic = time()\n",
    "\n",
    "# Numerical parameters\n",
    "Nx, Ny = 1024, 512\n",
    "Xmin, Xmax = 0, 2\n",
    "Ymin, Ymax = 0, 1\n",
    "Tmax = 5\n",
    "dt = 0.001\n",
    "\n",
    "# Diffusivity\n",
    "D = 0.0009\n",
    "\n",
    "# Global coordinates\n",
    "xc, dx = np.linspace(Xmin, Xmax, Nx, retstep = True)\n",
    "yc, dy = np.linspace(Ymin, Ymax, Ny, retstep = True)\n",
    "\n",
    "# Check stability condition for this method\n",
    "assert D*dt/dx**2 < 0.25\n",
    "\n",
    "# Infer local coordinates from rank, splitting the array along the x-axis only\n",
    "cells_per_rank = int(Nx/MPI_size)\n",
    "\n",
    "if MPI_size > 1:\n",
    "    # Treat first and last rank separately\n",
    "    if MPI_rank == 0:\n",
    "        x_local = xc[ : cells_per_rank + 1]\n",
    "    elif MPI_rank == MPI_size - 1:\n",
    "        x_local = xc[MPI_rank * cells_per_rank - 1 : ]\n",
    "    else:\n",
    "        x_local = xc[MPI_rank * cells_per_rank - 1 : (1+MPI_rank)*cells_per_rank + 1]\n",
    "else:\n",
    "    x_local = xc\n",
    "\n",
    "y_local = yc\n",
    "\n",
    "print(f'This is rank: {MPI_rank} of {MPI_size}. Local x-axis is from {x_local[0]} to {x_local[-1]}')\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "# Initial concentration (gaussian)\n",
    "x0, y0 = 1.0, 0.5\n",
    "sx, sy = 0.05, 0.05\n",
    "\n",
    "# local grid points\n",
    "gridy, gridx = np.meshgrid(y_local, x_local)\n",
    "# Create concentration field for local cells only\n",
    "C0 = np.exp(-(gridx - x0)**2/(2*sx**2) - (gridy-y0)**2/(2*sy**2))\n",
    "\n",
    "# Work array\n",
    "C = C0.copy()\n",
    "\n",
    "\n",
    "# Loop over time\n",
    "Nt = int(Tmax/dt)\n",
    "for i in range(Nt):\n",
    "    # Update interior points\n",
    "    C = FTCS(C, D, dt, dx)\n",
    "\n",
    "    # Only communicate if there is more than one rank\n",
    "    if MPI_size > 1:\n",
    "        # Exchange halo with neighbours\n",
    "        # First set up non-blocking receives\n",
    "        if MPI_rank == 0:\n",
    "            # Set up non-blocking receive from the right\n",
    "            comm.Irecv(C[-1,:], source = MPI_rank + 1)\n",
    "        elif MPI_rank == MPI_size - 1:\n",
    "            # Set up non-blocking receive from the left\n",
    "            comm.Irecv(C[ 0,:], source = MPI_rank - 1)\n",
    "        else:\n",
    "            # Set up non-blocking receive from both the left and the right\n",
    "            comm.Irecv(C[ 0,:], source = MPI_rank - 1)\n",
    "            comm.Irecv(C[-1,:], source = MPI_rank + 1)\n",
    "\n",
    "        # Then, set up blocking sends\n",
    "        if MPI_rank == 0:\n",
    "            # Set up blocking send to the right\n",
    "            comm.Send(C[-2,:], dest = MPI_rank + 1)\n",
    "        elif MPI_rank == MPI_size - 1:\n",
    "            # Set up blocking send to the left\n",
    "            comm.Send(C[ 1,:], dest = MPI_rank - 1)\n",
    "        else:\n",
    "            # Set up non-blocking send to both the left and the right\n",
    "            comm.Send(C[ 1,:], dest = MPI_rank - 1)\n",
    "            comm.Send(C[-2,:], dest = MPI_rank + 1)\n",
    "\n",
    "        comm.Barrier()\n",
    "\n",
    "print(f'This is rank: {MPI_rank} of {MPI_size}, finished computations')\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Finally, gather results on rank 0, and store to file\n",
    "if MPI_rank == 0:\n",
    "    C0_global = np.zeros((MPI_size, cells_per_rank, Ny))\n",
    "    C_global = np.zeros((MPI_size, cells_per_rank, Ny))\n",
    "else:\n",
    "    C0_global = None\n",
    "    C_global = None\n",
    "\n",
    "# Create a view of the local array that excludes boundary cells\n",
    "if MPI_rank == 0:\n",
    "    C0_local = C0[:-1, :]\n",
    "    C_local = C[:-1, :]\n",
    "elif MPI_rank == MPI_size -1:\n",
    "    C0_local = C0[1:, :]\n",
    "    C_local = C[1:, :]\n",
    "else:\n",
    "    C0_local = C0[1:-1, :]\n",
    "    C_local = C[1:-1, :]\n",
    "\n",
    "# Only communicate if there is more than one rank\n",
    "if MPI_size > 1:\n",
    "    # Collective communication\n",
    "    comm.Gather(C_local, C_global, root = 0)\n",
    "    comm.Gather(C0_local, C0_global, root = 0)\n",
    "else:\n",
    "    # If there is only one rank, store the entire array\n",
    "    C0_global = C0\n",
    "    C_global = C\n",
    "\n",
    "if MPI_rank == 0:\n",
    "    np.save('C0_MPI.npy', C0_global.reshape(Nx, Ny))\n",
    "    np.save('C_MPI.npy', C_global.reshape(Nx, Ny))\n",
    "    toc = time()\n",
    "    print(f'Simulation took {toc - tic:.5f} seconds')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
